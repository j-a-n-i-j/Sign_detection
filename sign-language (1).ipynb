{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\n# Define paths\ntrain_dir = '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train'\ntest_dir = '/kaggle/input/asl-alphabet/asl_alphabet_test/asl_alphabet_test'\n\n# Image data generator for training and testing\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Load and preprocess training data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(64, 64),  # Adjust target size based on your needs\n    color_mode='rgb',\n    batch_size=32,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(64, 64),\n    color_mode='rgb',\n    batch_size=32,\n    class_mode='categorical',\n    subset='validation'\n)\n\n# Load and preprocess testing data\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(64, 64),\n    color_mode='rgb',\n    batch_size=32,\n    class_mode='categorical'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T13:37:24.634118Z","iopub.execute_input":"2024-08-12T13:37:24.634806Z","iopub.status.idle":"2024-08-12T13:37:50.974978Z","shell.execute_reply.started":"2024-08-12T13:37:24.634773Z","shell.execute_reply":"2024-08-12T13:37:50.973760Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found 69600 images belonging to 29 classes.\nFound 17400 images belonging to 29 classes.\nFound 0 images belonging to 0 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n    MaxPooling2D((2, 2)),\n    \n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    \n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(29, activation='softmax')  # Assuming 29 classes for ASL alphabet\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=10\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T13:38:23.874376Z","iopub.execute_input":"2024-08-12T13:38:23.874822Z","iopub.status.idle":"2024-08-12T14:19:54.434232Z","shell.execute_reply.started":"2024-08-12T13:38:23.874787Z","shell.execute_reply":"2024-08-12T14:19:54.433223Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 117ms/step - accuracy: 0.2400 - loss: 2.5627 - val_accuracy: 0.7110 - val_loss: 0.8889\nEpoch 2/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 113ms/step - accuracy: 0.7563 - loss: 0.6722 - val_accuracy: 0.7991 - val_loss: 0.6990\nEpoch 3/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 114ms/step - accuracy: 0.8678 - loss: 0.3733 - val_accuracy: 0.8282 - val_loss: 0.6359\nEpoch 4/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 113ms/step - accuracy: 0.9112 - loss: 0.2638 - val_accuracy: 0.8217 - val_loss: 0.6746\nEpoch 5/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 113ms/step - accuracy: 0.9335 - loss: 0.2035 - val_accuracy: 0.8323 - val_loss: 0.6744\nEpoch 6/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 113ms/step - accuracy: 0.9460 - loss: 0.1675 - val_accuracy: 0.8471 - val_loss: 0.6209\nEpoch 7/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 114ms/step - accuracy: 0.9548 - loss: 0.1426 - val_accuracy: 0.8507 - val_loss: 0.6082\nEpoch 8/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 114ms/step - accuracy: 0.9615 - loss: 0.1258 - val_accuracy: 0.8491 - val_loss: 0.8288\nEpoch 9/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 112ms/step - accuracy: 0.9640 - loss: 0.1163 - val_accuracy: 0.8421 - val_loss: 0.8362\nEpoch 10/10\n\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 112ms/step - accuracy: 0.9657 - loss: 0.1119 - val_accuracy: 0.8489 - val_loss: 0.9448\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('asl_model_with_errors.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T14:27:42.382123Z","iopub.execute_input":"2024-08-12T14:27:42.382600Z","iopub.status.idle":"2024-08-12T14:27:42.445874Z","shell.execute_reply.started":"2024-08-12T14:27:42.382568Z","shell.execute_reply":"2024-08-12T14:27:42.444947Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}